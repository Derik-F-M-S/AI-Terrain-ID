{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hy1LtTafbcMr"
   },
   "source": [
    "# Competition Project: Terrain identification from accelerometer and gyroscope data using a cnn-lstm neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-19T23:24:51.231682Z",
     "iopub.status.busy": "2021-11-19T23:24:51.231111Z",
     "iopub.status.idle": "2021-11-19T23:24:51.237020Z",
     "shell.execute_reply": "2021-11-19T23:24:51.235671Z",
     "shell.execute_reply.started": "2021-11-19T23:24:51.231641Z"
    },
    "id": "DSzy3hUi7wvi",
    "outputId": "29108460-008b-42b4-9510-d95a01c1cb4d"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-19T23:24:57.083909Z",
     "iopub.status.busy": "2021-11-19T23:24:57.083288Z",
     "iopub.status.idle": "2021-11-19T23:24:57.089941Z",
     "shell.execute_reply": "2021-11-19T23:24:57.089134Z",
     "shell.execute_reply.started": "2021-11-19T23:24:57.083855Z"
    },
    "id": "0aL-vaFX7yD9",
    "outputId": "4cdbc18d-b62c-41d8-d5d8-0c738877919f"
   },
   "outputs": [],
   "source": [
    "flag_cuda = torch.cuda.is_available()\n",
    "\n",
    "if not flag_cuda:\n",
    "    print('Using CPU')\n",
    "else:\n",
    "    print('Using GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-19T23:24:55.334728Z",
     "iopub.status.busy": "2021-11-19T23:24:55.334093Z",
     "iopub.status.idle": "2021-11-19T23:24:55.346129Z",
     "shell.execute_reply": "2021-11-19T23:24:55.345051Z",
     "shell.execute_reply.started": "2021-11-19T23:24:55.334685Z"
    },
    "id": "uIUfRvT_74ID",
    "outputId": "08b36dd6-8890-4036-f99c-77ee86059ed0"
   },
   "outputs": [],
   "source": [
    "#moving into the directory with the project files\n",
    "%cd /kaggle/input/comp-proj/ECE542_fa2021_Project_TerrainRecognition/ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading in the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-19T22:06:37.255909Z",
     "iopub.status.busy": "2021-11-19T22:06:37.255472Z",
     "iopub.status.idle": "2021-11-19T22:12:41.286407Z",
     "shell.execute_reply": "2021-11-19T22:12:41.285665Z",
     "shell.execute_reply.started": "2021-11-19T22:06:37.255872Z"
    },
    "id": "KhBIRzH97eg8",
    "outputId": "5c5f4a17-6212-4d17-c45a-299e8ec2949c"
   },
   "outputs": [],
   "source": [
    "#these are the number of sessions that each subject did\n",
    "sub1ses= 8\n",
    "sub2ses= 5\n",
    "sub3ses= 3\n",
    "sub4ses= 2\n",
    "sub5ses= 3\n",
    "sub6ses= 3\n",
    "sub7ses= 4\n",
    "sub8ses= 1\n",
    "#list to store the training and testing data\n",
    "Training= []\n",
    "fileID=0\n",
    "column_list = ['ax','ay','az','gx','gy','gz','subject_id', 'time', 'label']\n",
    "Session_Data = pd.DataFrame([], columns = column_list)\n",
    "\n",
    "\n",
    "\n",
    "#array that conntains the number of sessions per person to itterate throgh the files\n",
    "num_ses_for_sub= [0,sub1ses, sub2ses, sub3ses, sub4ses, sub5ses, sub6ses, sub7ses, sub8ses]\n",
    "#in case we want to implement another method for loading the data \n",
    "method_load= 0\n",
    "#perTrain= 0.8\n",
    "\n",
    "if method_load==0:\n",
    "    for numsubjects in range(1,9):\n",
    "        for sessionID in range(1,num_ses_for_sub[numsubjects]+1):\n",
    "            print('Reading:./TrainingData/subject_{sub_id:03}_{sess_id:02}__x.csv'.format(sub_id=numsubjects,sess_id=sessionID))\n",
    "            X=pd.read_csv('./TrainingData/subject_{sub_id:03}_{sess_id:02}__x.csv'.format(sub_id=numsubjects,sess_id=sessionID),names=('ax','ay','az','gx','gy','gz'))\n",
    "            Y=pd.read_csv('./TrainingData/subject_{sub_id:03}_{sess_id:02}__y.csv'.format(sub_id=numsubjects,sess_id=sessionID),names=('label',))\n",
    "            x_t=pd.read_csv('./TrainingData/subject_{sub_id:03}_{sess_id:02}__x_time.csv'.format(sub_id=numsubjects,sess_id=sessionID), names= ('time', ))\n",
    "            y_t=pd.read_csv('./TrainingData/subject_{sub_id:03}_{sess_id:02}__y_time.csv'.format(sub_id=numsubjects,sess_id=sessionID), names= ('time', ))\n",
    "            subject_ID= [numsubjects] * len(x_t)\n",
    "            \n",
    "\n",
    "            X.insert(X.shape[1], 'subject_id', subject_ID)\n",
    "            X.insert(X.shape[1],'time', x_t)\n",
    "            Y.insert(0,'time', y_t)\n",
    "            #upsample the labels\n",
    "            X.insert(X.shape[1], 'label', Y.label[len(Y) - 1])\n",
    "            ii = 0\n",
    "            with pd.option_context('mode.chained_assignment', None):\n",
    "                for jj in range(0, len(X)):\n",
    "                    X.label[jj] = Y.label[ii]\n",
    "                    while ((ii < (len(Y) - 1)) and (X.time[jj] >= Y.time[ii + 1])):\n",
    "                        ii += 1\n",
    "            Session_Data = pd.concat([Session_Data, X], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing modules for the preprocessing of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-19T22:12:41.288384Z",
     "iopub.status.busy": "2021-11-19T22:12:41.288125Z",
     "iopub.status.idle": "2021-11-19T22:12:42.255358Z",
     "shell.execute_reply": "2021-11-19T22:12:42.254652Z",
     "shell.execute_reply.started": "2021-11-19T22:12:41.288351Z"
    },
    "id": "_9zlaE5gbbJ2",
    "outputId": "791a34f1-70b8-486e-a1e2-02b0d15dc69e"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob \n",
    "import numpy as np\n",
    "import random\n",
    "import sklearn.metrics\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from scipy import stats\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from statsmodels import robust\n",
    "from scipy import  stats\n",
    "from scipy.stats import entropy\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-19T23:25:01.767756Z",
     "iopub.status.busy": "2021-11-19T23:25:01.767119Z",
     "iopub.status.idle": "2021-11-19T23:25:01.786625Z",
     "shell.execute_reply": "2021-11-19T23:25:01.784666Z",
     "shell.execute_reply.started": "2021-11-19T23:25:01.767710Z"
    }
   },
   "outputs": [],
   "source": [
    "Session_Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-19T22:19:22.437701Z",
     "iopub.status.busy": "2021-11-19T22:19:22.437412Z",
     "iopub.status.idle": "2021-11-19T22:19:22.554972Z",
     "shell.execute_reply": "2021-11-19T22:19:22.554092Z",
     "shell.execute_reply.started": "2021-11-19T22:19:22.437667Z"
    }
   },
   "outputs": [],
   "source": [
    "x_data=Session_Data[Session_Data.columns[:6]]\n",
    "x_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-19T22:19:25.478813Z",
     "iopub.status.busy": "2021-11-19T22:19:25.478157Z",
     "iopub.status.idle": "2021-11-19T22:19:27.787325Z",
     "shell.execute_reply": "2021-11-19T22:19:27.786514Z",
     "shell.execute_reply.started": "2021-11-19T22:19:25.478776Z"
    },
    "id": "gX0Wn6CrcOpE"
   },
   "outputs": [],
   "source": [
    "x_data=Session_Data[Session_Data.columns[:6]]\n",
    "scaler = RobustScaler()\n",
    "\n",
    "scaler = scaler.fit(x_data)\n",
    "\n",
    "Session_Data.loc[:, Session_Data.columns[:6]] = scaler.transform((Session_Data[Session_Data.columns[:6]]).to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-19T22:19:28.027183Z",
     "iopub.status.busy": "2021-11-19T22:19:28.026596Z",
     "iopub.status.idle": "2021-11-19T22:19:28.040128Z",
     "shell.execute_reply": "2021-11-19T22:19:28.039448Z",
     "shell.execute_reply.started": "2021-11-19T22:19:28.02715Z"
    }
   },
   "outputs": [],
   "source": [
    "Session_Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-19T22:19:31.31726Z",
     "iopub.status.busy": "2021-11-19T22:19:31.316987Z",
     "iopub.status.idle": "2021-11-19T22:19:31.324294Z",
     "shell.execute_reply": "2021-11-19T22:19:31.323398Z",
     "shell.execute_reply.started": "2021-11-19T22:19:31.317228Z"
    },
    "id": "HrHospRncP8P"
   },
   "outputs": [],
   "source": [
    "def extract_windows(X, Y, Window_Size=40, stride=1):\n",
    "    X_windows, Y_windows = [], []\n",
    "      \n",
    "    for i in range(0, len(X) - Window_Size, stride):\n",
    "        u = X.iloc[i:(i + Window_Size)].values\n",
    "        labels = Y.iloc[i: i + Window_Size]\n",
    "        X_windows.append(u)\n",
    "        Y_windows.append(stats.mode(labels)[0][0])\n",
    "    return np.array(X_windows), np.array(Y_windows).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-19T22:19:33.570493Z",
     "iopub.status.busy": "2021-11-19T22:19:33.570048Z",
     "iopub.status.idle": "2021-11-19T22:24:24.521426Z",
     "shell.execute_reply": "2021-11-19T22:24:24.52067Z",
     "shell.execute_reply.started": "2021-11-19T22:19:33.570453Z"
    },
    "id": "j-dm4Q0XcVZf"
   },
   "outputs": [],
   "source": [
    "Window_Size = 40  # Window Size\n",
    "stride = 1\n",
    "x_data=Session_Data[Session_Data.columns[:6]]\n",
    "y_data=Session_Data.label\n",
    "X_data, Y_data = extract_windows(x_data,y_data,Window_Size,stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-19T22:24:24.523177Z",
     "iopub.status.busy": "2021-11-19T22:24:24.522942Z",
     "iopub.status.idle": "2021-11-19T22:24:25.189057Z",
     "shell.execute_reply": "2021-11-19T22:24:25.188318Z",
     "shell.execute_reply.started": "2021-11-19T22:24:24.523147Z"
    }
   },
   "outputs": [],
   "source": [
    "rus = RandomUnderSampler(sampling_strategy='not minority', random_state=1)\n",
    "rus.fit_resample(X_data[:,:,0], Y_data)\n",
    "X_data = X_data[rus.sample_indices_]\n",
    "Y_data = Y_data[rus.sample_indices_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-19T22:24:25.190689Z",
     "iopub.status.busy": "2021-11-19T22:24:25.190227Z",
     "iopub.status.idle": "2021-11-19T22:24:25.195084Z",
     "shell.execute_reply": "2021-11-19T22:24:25.194196Z",
     "shell.execute_reply.started": "2021-11-19T22:24:25.190648Z"
    },
    "id": "MdXCIfwCcfma"
   },
   "outputs": [],
   "source": [
    "X_data = np.expand_dims(X_data, axis=1)#insert a channel dimension for the conv layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-19T22:24:25.197775Z",
     "iopub.status.busy": "2021-11-19T22:24:25.19751Z",
     "iopub.status.idle": "2021-11-19T22:24:25.206085Z",
     "shell.execute_reply": "2021-11-19T22:24:25.204595Z",
     "shell.execute_reply.started": "2021-11-19T22:24:25.197741Z"
    }
   },
   "outputs": [],
   "source": [
    "print(X_data.shape)\n",
    "print(Y_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing modules for the model creation and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-19T22:24:25.207997Z",
     "iopub.status.busy": "2021-11-19T22:24:25.207285Z",
     "iopub.status.idle": "2021-11-19T22:24:34.427395Z",
     "shell.execute_reply": "2021-11-19T22:24:34.4266Z",
     "shell.execute_reply.started": "2021-11-19T22:24:25.207956Z"
    },
    "id": "aNt8WNYEgSwy"
   },
   "outputs": [],
   "source": [
    "!pip install pyfiglet \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import mode\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from multiprocessing import cpu_count\n",
    "from pathlib import Path\n",
    "from tqdm.autonotebook import tqdm\n",
    "import os\n",
    "from pyfiglet import Figlet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the data into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-19T22:24:34.429426Z",
     "iopub.status.busy": "2021-11-19T22:24:34.429155Z",
     "iopub.status.idle": "2021-11-19T22:24:34.602394Z",
     "shell.execute_reply": "2021-11-19T22:24:34.601509Z",
     "shell.execute_reply.started": "2021-11-19T22:24:34.429388Z"
    },
    "id": "2-wbxSjychBr"
   },
   "outputs": [],
   "source": [
    "val_size = 0.2  # validation size (80:20 split )\n",
    "#encoding the labels\n",
    "enc = LabelEncoder()\n",
    "Y_data = enc.fit_transform(Y_data)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_data, Y_data, test_size=val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-19T22:24:34.604328Z",
     "iopub.status.busy": "2021-11-19T22:24:34.603932Z",
     "iopub.status.idle": "2021-11-19T22:24:34.610132Z",
     "shell.execute_reply": "2021-11-19T22:24:34.609111Z",
     "shell.execute_reply.started": "2021-11-19T22:24:34.604286Z"
    },
    "id": "puaANrPvdXuS"
   },
   "outputs": [],
   "source": [
    "def create_data_loaders(train_ds, valid_ds, bs=64, jobs=0):\n",
    "    train_dl = DataLoader(train_ds, bs, shuffle=True, num_workers=jobs,drop_last=True )\n",
    "    valid_dl = DataLoader(valid_ds, bs, shuffle=False, num_workers=jobs,drop_last=True)\n",
    "    return train_dl, valid_dl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-19T22:24:34.611804Z",
     "iopub.status.busy": "2021-11-19T22:24:34.611537Z",
     "iopub.status.idle": "2021-11-19T22:24:34.772861Z",
     "shell.execute_reply": "2021-11-19T22:24:34.7719Z",
     "shell.execute_reply.started": "2021-11-19T22:24:34.61177Z"
    },
    "id": "AGUSG8oXfyP5"
   },
   "outputs": [],
   "source": [
    "X_train, X_val = [torch.tensor(arr, dtype=torch.float32) for arr in (X_train, X_val)]\n",
    "y_train, y_val = [torch.tensor(arr, dtype=torch.long) for arr in (y_train, y_val)]\n",
    "train_ds = TensorDataset(X_train, y_train)\n",
    "valid_ds = TensorDataset(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the data into batches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-19T22:24:34.774625Z",
     "iopub.status.busy": "2021-11-19T22:24:34.774331Z",
     "iopub.status.idle": "2021-11-19T22:24:34.779462Z",
     "shell.execute_reply": "2021-11-19T22:24:34.778484Z",
     "shell.execute_reply.started": "2021-11-19T22:24:34.774573Z"
    },
    "id": "JWPhCt7Xdz19",
    "outputId": "ca4cc3c3-e6a4-4cca-cb22-f0b87b0e7a7a"
   },
   "outputs": [],
   "source": [
    "bs = 512\n",
    "trn_dl, val_dl = create_data_loaders(train_ds, valid_ds, bs, jobs=cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition \n",
    "4 2d convolution layers followed by an lstm layer and then a fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-20T00:40:40.585902Z",
     "iopub.status.busy": "2021-11-20T00:40:40.585336Z",
     "iopub.status.idle": "2021-11-20T00:40:40.598569Z",
     "shell.execute_reply": "2021-11-20T00:40:40.597706Z",
     "shell.execute_reply.started": "2021-11-20T00:40:40.585851Z"
    },
    "id": "24ZfTIdrxkTu"
   },
   "outputs": [],
   "source": [
    "class CNNAndLstm(nn.Module):    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv2d1= nn.Conv2d(in_channels=1, out_channels=120, kernel_size=(5,1))\n",
    "        self.conv2d2= nn.Conv2d(out_channels=120, kernel_size=(5,1),in_channels=120)\n",
    "        self.conv2d3= nn.Conv2d(out_channels=120, kernel_size=(5,1),in_channels=120 )\n",
    "        self.conv2d4= nn.Conv2d(out_channels=120, kernel_size=(5,1),in_channels=120 )\n",
    "        self.lstm1 = nn.LSTM(6*120, 128, batch_first=True)\n",
    "        self.fc = nn.Linear(128, 4)    \n",
    "    def forward(self, x):\n",
    "        Window=x.size(2)\n",
    "        bsn= x.size(0)\n",
    "        if flag_cuda: h01, c01 = torch.randn(1, bsn, 128).cuda(), torch.randn(1, bsn, 128).cuda()\n",
    "        if not flag_cuda: h01, c01 = torch.randn(1, bsn, 128), torch.randn(1, bsn, 128)\n",
    "        x=F.relu(self.conv2d1(x))\n",
    "        x=F.relu(self.conv2d2(x))\n",
    "        x=F.relu(self.conv2d3(x))\n",
    "        x=F.relu(self.conv2d4(x))\n",
    "        x= x.reshape(bsn,(Window-16), 6*120)\n",
    "        lstm_out, _,=self.lstm1(x,(h01,c01))\n",
    "        output = self.fc(lstm_out[:, -1, :])\n",
    "\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# making a directory to save the model files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-19T03:04:45.81259Z",
     "iopub.status.busy": "2021-11-19T03:04:45.812329Z",
     "iopub.status.idle": "2021-11-19T03:04:46.521222Z",
     "shell.execute_reply": "2021-11-19T03:04:46.520363Z",
     "shell.execute_reply.started": "2021-11-19T03:04:45.81256Z"
    }
   },
   "outputs": [],
   "source": [
    "!rm /kaggle/working/model -rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-19T22:24:34.798069Z",
     "iopub.status.busy": "2021-11-19T22:24:34.797793Z",
     "iopub.status.idle": "2021-11-19T22:24:35.4686Z",
     "shell.execute_reply": "2021-11-19T22:24:35.467695Z",
     "shell.execute_reply.started": "2021-11-19T22:24:34.798033Z"
    }
   },
   "outputs": [],
   "source": [
    "!mkdir /kaggle/working/model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model and saving the best ones as it trains "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-20T00:40:46.053372Z",
     "iopub.status.busy": "2021-11-20T00:40:46.052819Z",
     "iopub.status.idle": "2021-11-20T01:39:27.556903Z",
     "shell.execute_reply": "2021-11-20T01:39:27.555880Z",
     "shell.execute_reply.started": "2021-11-20T00:40:46.053333Z"
    },
    "id": "y5cc0KfFc95U",
    "outputId": "ad20dd71-b683-44f3-e6cb-bfed04d4d27b"
   },
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "n_epochs = 100\n",
    "best_accuracy = 0\n",
    "output_dim=4\n",
    "\n",
    "model = CNNAndLstm()\n",
    "if flag_cuda: model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "sched = None\n",
    "\n",
    "print(':::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\\n')\n",
    "print('-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\\n')\n",
    "\n",
    "f = Figlet(font='slant')\n",
    "print(f.renderText('Training Start!'))\n",
    "\n",
    "\n",
    "train_loss_list = []\n",
    "valid_loss_list = []\n",
    "for epoch in tqdm(range(1, n_epochs + 1)):\n",
    "    \n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    model.train()\n",
    "\n",
    "    for data, target in trn_dl:\n",
    "        if flag_cuda:\n",
    "              data, target = data.cuda(), target.cuda()\n",
    "        # Clearing the gradients of all optimized variables\n",
    "        opt.zero_grad()\n",
    "        #switching the order of the dimensions since the conv layers expects (N, C_{in}, H_{in}, W_{in})\n",
    "        #data= data.transpose(1,3)\n",
    "        #data=data.transpose(2,3)\n",
    "        # Forward pass: Computing predicted outputs\n",
    "        output = model(data)\n",
    "        # Calculating the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # Backward pass: compute gradient of loss with respect to parameters\n",
    "        loss.backward()\n",
    "        # Perform a single optimization step (parameter update)\n",
    "        opt.step()\n",
    "        # Update training loss\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "    train_loss_list.append(train_loss)\n",
    "    #set model to evaluation so that we can compute the validation loss\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    for x_val, y_val in val_dl:\n",
    "        x_val, y_val = [t.cuda() for t in (x_val, y_val)]\n",
    "        #x_val= x_val.transpose(1,3)\n",
    "        #x_val=x_val.transpose(2,3)\n",
    "        out = model(x_val)\n",
    "        prediction = F.log_softmax(out, dim=1).argmax(dim=1)\n",
    "        total += y_val.size(0)\n",
    "        correct += (prediction == y_val).sum().item()\n",
    "    \n",
    "        loss = criterion(out, y_val)\n",
    "        valid_loss += loss.item()*x_val.size(0)\n",
    "    valid_loss_list.append(valid_loss)\n",
    "\n",
    "    acc = correct / total\n",
    "\n",
    "    print('Epoch: {} \\tTraining Loss: {:.5f} \\tValidation Loss: {:.5f}'.format(epoch, train_loss, valid_loss))\n",
    "\n",
    "    if acc > best_accuracy:\n",
    "        best_accuracy = acc\n",
    "        !rm /kaggle/working/model/* -v\n",
    "        torch.save(model.state_dict(), f'/kaggle/working/model/bestcnn_{best_accuracy:2.3%}.pth')\n",
    "        print(f'Saving New Best Model With an accuracy of: {best_accuracy:2.3%}')\n",
    "    else:\n",
    "        print(f'Did not get a better model at epoch: {epoch}')\n",
    "print(':::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\\n')\n",
    "print('-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\\n')\n",
    "f = Figlet(font='slant')\n",
    "print(f.renderText('Training End!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the training and validation loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-20T01:39:27.559644Z",
     "iopub.status.busy": "2021-11-20T01:39:27.559106Z",
     "iopub.status.idle": "2021-11-20T01:39:27.797134Z",
     "shell.execute_reply": "2021-11-20T01:39:27.796440Z",
     "shell.execute_reply.started": "2021-11-20T01:39:27.559566Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs_list= np.arange(1,n_epochs+1,1)\n",
    "plt.plot(epochs_list, train_loss_list, epochs_list, valid_loss_list)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(['Training','Validation'])\n",
    "plt.title(\"Performance of Baseline Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Final predictions for the leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-20T01:57:30.342513Z",
     "iopub.status.busy": "2021-11-20T01:57:30.341797Z",
     "iopub.status.idle": "2021-11-20T01:57:30.351113Z",
     "shell.execute_reply": "2021-11-20T01:57:30.350306Z",
     "shell.execute_reply.started": "2021-11-20T01:57:30.342473Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#function to filter the data from the predictions using a sliding window\n",
    "def filter_out(pred_df): \n",
    "    WINDOW=20\n",
    "    arr=np.asarray(pred_df).squeeze()\n",
    "    for ii in range(0, len(arr)-WINDOW, 1):\n",
    "        z= arr[ii:ii+WINDOW]\n",
    "        mode_of_window= mode(z)[0]\n",
    "        c_val= arr[ii]\n",
    "    if arr[ii] != mode_of_window:\n",
    "        arr[ii]= int(mode_of_window) \n",
    "    \n",
    "    return arr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-20T01:57:49.306219Z",
     "iopub.status.busy": "2021-11-20T01:57:49.305952Z",
     "iopub.status.idle": "2021-11-20T01:59:10.696492Z",
     "shell.execute_reply": "2021-11-20T01:59:10.695654Z",
     "shell.execute_reply.started": "2021-11-20T01:57:49.306189Z"
    }
   },
   "outputs": [],
   "source": [
    "WINDOW = 40  # Window Size\n",
    "STRIDE = 1\n",
    "output_dim = 4\n",
    "model = CNNAndLstm()\n",
    "if flag_cuda: model = model.cuda()\n",
    "scale_columns=Session_Data.columns[:6]\n",
    "model_path = '/kaggle/working/model/'\n",
    "model_file= glob.glob(model_path + '*.pth')[0]\n",
    "if flag_cuda: model.load_state_dict(torch.load(model_file)) #path to the best model\n",
    "if not flag_cuda: model.load_state_dict(torch.load(model_file,map_location=torch.device('cpu')))  #path to the best model\n",
    "model.eval()\n",
    "dir_path = '/kaggle/input/comp-proj/ECE542_fa2021_Project_TerrainRecognition/TestData/'\n",
    "column_list = ['ax', 'ay', 'az', 'gx', 'gy', 'gz']\n",
    "df_test_data = pd.DataFrame([], columns = column_list)\n",
    "label_files=[]\n",
    "## Reading Test files \n",
    "for idx in glob.glob(dir_path + '*.csv'):\n",
    "    file_type = idx.split('.')[0].split('__')[1]\n",
    "\n",
    "    if file_type == 'x':\n",
    "        subject_name = idx.split('.')[0].split('__')[0].split('/')[-1]\n",
    "        x_file = idx.split('.')[0].split('__')[0] + '__' + file_type + '.csv'\n",
    "        df_x = pd.read_csv(x_file, names=column_list[:6])\n",
    "        \n",
    "        df_x = df_x.iloc[0:]\n",
    "        df_x.loc[:, column_list] = scaler.transform(df_x[column_list].to_numpy())\n",
    "        df_x.insert(df_x.shape[1], 'label', -1)        \n",
    "        ## Converting windows for test data \n",
    "        x_test, y_test = extract_windows(df_x[column_list],df_x.label,WINDOW,STRIDE )\n",
    "        #print(\"len(x_test)\", len(x_test))        \n",
    "        #Running inference on test data with bs of 128 just like the training data\n",
    "        bs=128\n",
    "        x_test = np.expand_dims(x_test, 1)\n",
    "        x_test = torch.tensor(x_test, dtype=torch.float32)#convert to torch tensor \n",
    "        y_test = torch.tensor(y_test, dtype=torch.long)#convert to torch tensor \n",
    "        \n",
    "        test_ds= TensorDataset(x_test,y_test) #create a dataset with this data \n",
    "        test_dl = DataLoader(test_ds, bs, shuffle=False, num_workers=cpu_count(),drop_last=False )#not dropping the last batch so we get all the test values\n",
    "        predictions_final= pd.DataFrame(columns=['label'])\n",
    "        for x_test, y_test in test_dl:\n",
    "            \n",
    "            if flag_cuda: x_test, y_test=  x_test.cuda(), y_test.cuda() #move to GPU\n",
    "            out = model(x_test)\n",
    "            predictions = F.log_softmax(out, dim=1).argmax(dim=1)\n",
    "            predictions = list(predictions)\n",
    "            predictions = np.asarray(predictions)\n",
    "            predictions = pd.DataFrame(predictions, columns=['label'])\n",
    "            predictions_final= predictions_final.append(predictions, ignore_index=True)\n",
    "        #add the last window as just zero labels\n",
    "        last_window= np.zeros(WINDOW-1)\n",
    "        last_window_df= pd.DataFrame(last_window,columns=['label'])\n",
    "        predictions_final= last_window_df.append(predictions_final, ignore_index=True)\n",
    "\n",
    "        #downsample\n",
    "        predictions_final_downsampled = predictions_final['label'].rolling(window=4, min_periods=1).apply(lambda x: mode(x)[0])[::4]\n",
    "\n",
    "        predictions_final_downsampled = pd.DataFrame(predictions_final_downsampled.values)\n",
    "        #filter the lables to get cleaner output trying to remove any short spikes of a label\n",
    "        filtered_arr= filter_out(predictions_final_downsampled)\n",
    "        predictions_final_downsampled = pd.DataFrame(filtered_arr)\n",
    "        #print(pred_df)\n",
    "        print(\"Saving file:\", subject_name, \"__y.csv\")\n",
    "        predictions_final_downsampled.to_csv('/kaggle/working/' + subject_name + '__y.csv', index=False, columns=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-20T02:01:47.524166Z",
     "iopub.status.busy": "2021-11-20T02:01:47.523471Z",
     "iopub.status.idle": "2021-11-20T02:01:47.803125Z",
     "shell.execute_reply": "2021-11-20T02:01:47.802419Z",
     "shell.execute_reply.started": "2021-11-20T02:01:47.524121Z"
    }
   },
   "outputs": [],
   "source": [
    "#plotting the last prediction that was generated\n",
    "print(len(pred_df))\n",
    "lablesarr=np.asarray(pred_df).squeeze()\n",
    "print(lablesarr)\n",
    "timelist= np.arange(0,9473,1)\n",
    "plt.figure(figsize=(30,10))\n",
    "plt.plot(timelist, lablesarr)\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"label\")\n",
    "plt.legend(['Prediction'])\n",
    "plt.title(\"Labels\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
